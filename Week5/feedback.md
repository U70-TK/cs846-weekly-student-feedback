# Week 5 Feedback: Code Summarization / Comprehension

**Authors:** [Neel Sanjaybhai Faganiya, Ibrahim Mohammed Sayem, Felix Wang]

## 1. Counterexample Problems

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example

Find the related files in the `artifacts` folder.

## Example Problems

### Problem A: Code Documentation with Implementation-Dependent Contracts

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example

**Task Description:** 

Document the `findShortestPath` function in the provided code. The function is part of a pathfinding system where the algorithm choice (BFS), data structure (queue), and traversal order are essential to the correctness guarantees. Your task is to create documentation that explains what the function does, its parameters, return value, guarantees, and any necessary details for the reviewer or developer.

**Starter Code:**

The code for this task and its related files can be found in the `artifacts` folder. The starter code is located at `artifacts/graph_traversal.cpp`.


---

### Problem B: Config-Intensive Code Documentation

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example

**Task Description:**

Document the provided project repository. The `main.py` file contains the main function of the ``medical_reasoning_SFT`` project, which calls an open source LLM Supervised Fine-tuning framework `axolotl` on the `OpenMed/Medical-Reasoning-SFT-Trinity-Mini` dataset using LoRA adapters. Your task is to create documentation that explains what this file does, and/or any necessary technical details for the developers. 

**Starter Code:**

Find the related project in the `Week5/artifacts/medical_reasoning_SFT/` folder. Consider providing a documentation template for the summary. 

---

### Problem C: 

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example


**Task Description:**

You are provided with a Python implementation of a Ride-Sharing Management System. Your task is to create comprehensive, developer-ready documentation (docstrings/Doxygen comments) that explains the functionality, what the code does, and all the necessary technical details that developers need to understand and maintain the system. 

**Starter Code:**

The code for the task, along with its related files, can be found in the artifacts folder. Find the related projects in the Week5/artifacts/ride_sharing_management_system.py.Consider providing a documentation template for the summary. 

---

### 2. Guidelines that didn't work

---

#### Guideline 4: Document Purpose and Contract, Not Implementation

**Prompt and Context:** 

Document the findShortestPath function following Guideline 4: Focus on what it accomplishes (its contract), not how it works internally. Describe the purpose, parameters, return value, and guaranteesâ€”avoid implementation details like algorithms or data structures.

**Task:**  

The task is to document the `findShortestPath` function in the provided code. The function is part of a pathfinding system where the algorithm choice (BFS), data structure (queue), and traversal order are essential to the correctness guarantees. The documentation should explains what the function does, its parameters, return value, guarantees, and any necessary details for the reviewer or developer.

**Solution or Output generated by Github Copilot:**

```cpp
/**
 * @brief Finds the shortest path from a start node to a target node in the graph.
 *
 * This function computes the shortest path in terms of the number of edges, while ensuring
 * that the path respects the priority constraints of the nodes. The path must visit nodes
 * in non-decreasing priority order (where lower priority values indicate higher importance).
 *
 * @param start The ID of the starting node. Must be a valid node ID within the graph.
 * @param target The ID of the target node. Must be a valid node ID within the graph.
 * @return A vector of node IDs representing the shortest path from start to target, including both start and target nodes. Returns an empty vector if no path exists.
 * @note The returned path guarantees the minimum number of edges and maintains priority ordering.
 * @note If start equals target, returns a vector containing only the start node.
 * @note Thread-safe for concurrent reads; exclusive access required for modifications to the graph.
 */
```

**Why this is bad:**
- **Missing algorithm specification**: Doesn't mention BFS, so developers might use DFS
- **Missing performance info**: No complexity analysis
- **No data structure info**: Doesn't mention queue requirement
- **Could lead to incorrect usage**: Developer might think any path-finding algorithm works

---

#### Guideline 3: Provide a Documentation Template for the Summary

**Prompt and Context:** 
```
Read the Python project stored at `./Week5/artifacts/medical_reasoning_SFT`, then document the ``main.py`` file in the provided project repository. This file contains the main function of the ``medical_reasoning_SFT`` project, which calls an open source LLM Supervised Fine-tuning framework `axolotl` on the `OpenMed/Medical-Reasoning-SFT-Trinity-Mini` dataset using LoRA adapters. Your task is to create documentation that explains what this file does, and/or any necessary technical details for the developers.


<rule>
Our project's documentation style:
@brief [One-line description ending with period]

[Optional detailed description, 1-2 sentences]

@param [ name] [Purpose and constraints, not just type]
@return [What the return value represents]

@pre [Precondition if any]
@exceptsafe [No-throw | Strong | Basic]
</rule>

<example>
def pool_free(pool, ptr):
    """
    @brief Frees previously allocated memory back to the pool.

    Releases memory that was allocated from the given memory pool.
    The pointer must refer to a valid allocation obtained via pool_alloc().

    @param pool Memory pool to free memory back to
    @param ptr Pointer to memory previously returned by pool_alloc()

    @pre pool must be initialized via pool_init()
    @pre ptr must be a valid pointer allocated from this pool
    @exceptsafe No-throw guarantee
    """
    pass
</example>
```

**Task:**

The related problem to this task is `Problem B: Config-Intensive Code Documentation`. You should read the Python project stored at `./Week5/artifacts/medical_reasoning_SFT` (with coding agent's help), then document the ``main.py`` file in the provided project repository. 

**Solution or Output generated by Github Copilot:**

```python
def train(args):
    """
    @brief Build and run Axolotl training or preprocessing command from CLI args.

    Uses the repository `axolotl_configs/` directory to resolve a config by
    name (`--config`) or accepts a direct path to a YAML file
    (`--config-path`). Can list available configs, run preprocessing only,
    and run under `accelerate launch` for multi-GPU setups. The function
    invokes the external Axolotl CLI via subprocess and exits with the
    command's return code on completion.

    @param args Parsed argparse.Namespace containing options:
                 - config: optional named config identifier ("model/name").
                 - config_path: optional path to a YAML config file.
                 - list: list available configs and exit.
                 - accelerate: use `accelerate launch` instead of `python -m`.
                 - preprocess_only: run preprocessing instead of training.
                 - debug: append `--debug` to Axolotl invocation.
    @return None (function calls `sys.exit` on completion).

    @pre `axolotl` must be installed (and `accelerate` if using `--accelerate`).
    @exceptsafe Basic
    """
    ...

def main():
    """
    @brief Command-line entry point: parse args and invoke `train()`.

    Defines the CLI flags for selecting a configuration or config path,
    listing available configs, choosing `accelerate` vs local run, and
    enabling preprocessing-only or debug mode. Parses arguments and
    forwards them to `train()` for execution.

    @return None
    @pre None
    @exceptsafe Basic
    """
    ...
```

**Why this is bad:**

1. This guideline implicitly implies that only source code files need to be documented. And for most programming languages, it is possible to create a function documentation template for LLMs or coding agents to follow. However, in this example, the `main.py` file is just a thin wrapper for parsing the CLI arguments and dispatching to the Axolotl framework. It is the orchestration layer, but not the behaviour layer of the project. 

2. It happens to a lot of Machine-Learning or Large Language Model (LLM) based projects that the actual system behaviour lives entirely in the YAML files: `axolotl_configs/llama2-chat/lora-openmed_Trinity-Mini.yaml` and `axolotl_configs/qwen2.5-7B/lora-openmed_Trinity-Mini.yaml`. 

3. Documenting with such a code summarization template would create procedural correctness, but semantic emptiness. One may read the documentation and have absolutely no idea of what model is trained, what data format is used, or what dataset is used. 

---

#### Guideline 6: Develop a global plan for the entire repository

You are provided with a Python implementation of a Ride-Sharing Management System. Your task is to create comprehensive, developer-ready documentation (docstrings/Doxygen comments) that explains the functionality, what the code does, and all the necessary technical details that developers need to understand and maintain the system using guideline 6. 

**Task:** 

The code for the task, along with its related files, can be found in the artifacts folder. Find the related projects in the Week5/artifacts/ride_sharing_management_system.py.

**Solution or Output generated by GitHub Copilot:**

"""@brief Simple ride-sharing module.

A lightweight, example ride-sharing implementation used for demos.
This module contains basic in-memory models for drivers, passengers, and
rides and provides simple helper methods to request and complete rides.
"""
def calculate_fare(self, base_rates: dict, surge_multiplier: float = 1.0):
        """
        @brief Calculate the ride fare.

        Computes a fare using the provided rate table and optional surge.

        @param base_rates Mapping from VehicleType to rate components.
        @param surge_multiplier Multiplier to apply to final fare.
        @return Rounded fare as a float.
        """
def request_ride(self, passenger_id: str, pickup: Location, 
                     dropoff: Location, vehicle_type: VehicleType) -> Optional[Ride]:
        """
        @brief Create and register a ride request.

        Creates a new ride for the passenger and tries to assign a driver.

        @param passenger_id Registered passenger identifier.
        @param pickup Pickup Location object.
        @param dropoff Dropoff Location object.
        @param vehicle_type Requested vehicle category.
        @return Ride object on success, or None if the passenger is unknown.
        """
 def _find_best_driver(self, ride: Ride) -> Optional[Driver]:
        """
        @brief Find the best available driver for the given ride.

        Filters and scores available drivers to pick the best match.

        @param ride Ride object to match a driver for.
        @return Driver instance or None if no candidate found.
        """

...

**Why this is bad:**

1.	Scope Mismatch: The given problem is a single, self-contained Python file (approximately 300 lines), not a multi-file repository. There are no multiple modules, packages, or distributed components to plan for.
2.	Unnecessary Overhead: Creating a repository-level plan would be overkill and waste effort when the entire codebase is already cohesive and contained in one file.
3.	Clear Structure Already Exists: The code already has a logical structure (enums, data classes, main system class) that doesn't require repository-level planning.
4.	Distracts from Core Task: Time spent on global repository planning detracts from the actual task of documenting the existing code's functionality and technical details.
5.	Not Scalable to Single-File Context: This guideline assumes a complex, multi-component architecture, which doesn't apply here.

...

---

### 3. New and Updated Guidelines that worked

#### Guideline 1: Include Implementation Details When They Are Part of the Contract

**Description**

Focus documentation on what the function accomplishes and its guarantees, but include implementation details when those details are essential to the contract. Document algorithm choices, data structures, and performance characteristics when they directly impact the function's guarantees, correctness, or when understanding the implementation is necessary to understand the contract.

**Reasoning**

Traditional documentation guidelines emphasize documenting only the "what" (purpose and contract) while avoiding the "how" (implementation details). However, when algorithm choice guarantees specific properties (e.g., BFS guarantees shortest path while DFS does not), when data structures are required for correctness (e.g., FIFO queue for level-order traversal), or when performance characteristics are part of the API contract, these implementation details become essential to understanding and correctly using the function. Omitting such details can lead to misuse, incorrect assumptions, or misunderstanding of the function's guarantees.

**Prompt and Context:** 

Document this function. Describe its purpose, parameters, return value, and guarantees. Include implementation details that are part of the contract: analyze the code to identify which algorithm, data structures, or implementation choices are essential to the guarantees provided, and document those along with how they ensure the contract. Include performance characteristics if they're part of the API contract.

**Task:** 

The task is to document the `findShortestPath` function in the provided code. The function is part of a pathfinding system where the algorithm choice (BFS), data structure (queue), and traversal order are essential to the correctness guarantees. The documentation should explains what the function does, its parameters, return value, guarantees, and any necessary details for the reviewer or developer.

**Solution or Output generated by Github Copilot:**

```cpp
/**
 * @brief Finds the shortest path between two nodes using priority-aware BFS.
 *
 * This function implements breadth-first search (BFS) to guarantee finding the
 * shortest path in terms of number of edges. Within each level of the BFS
 * traversal, nodes are processed in priority order (lower priority values
 * indicate higher importance). This ensures that among all shortest paths,
 * the one with the best priority ordering is selected.
 *
 * @param start The ID of the starting node.
 * @param target The ID of the target node.
 * @return A vector of node IDs representing the shortest path from start to target, respecting priority constraints. Returns an empty vector if no path exists or if start equals target (in which xscase {start} is returned).
 *
 * @note Algorithm: BFS (breadth-first search) is used instead of DFS to guarantee the shortest path. DFS could find a path but not necessarily the shortest one.
 *
 * @note Data Structure: A FIFO queue maintains level-order traversal, ensuring all nodes at depth N are processed before depth N+1. This is critical for the shortest path guarantee.
 *
 * @note Priority Processing: Within each level, nodes are sorted by priority (ascending order, lower priority first) before processing. This ensures that if multiple paths of equal length exist, the path visiting higher-priority nodes earlier is chosen.
 *
 * @note Time Complexity: O(V + E) for BFS traversal, where V is the number of vertices and E is the number of edges. Additional O(V log V) in the worst case if all nodes are at the same level and need sorting, but typically levels are small, making this negligible.
 *
 * @note Space Complexity: O(V) for the queue, visited set, and parent array.
 *
 * @warning Do not replace BFS with DFS, as DFS does not guarantee shortest paths. The queue-based level-order processing is essential for correctness. Changing the data structure to a stack would break the level-order guarantee.
 *
 * @warning The priority ordering is applied per level, not globally across the entire path. This means the path may not have monotonically decreasing priorities, but within each BFS level, higher-priority nodes are processed first.
 */
```

**Why this is better:**

- Includes algorithm specification (BFS) because it guarantees shortest path
- Documents data structure (queue) because it's required for correctness
- Explains performance characteristics (part of API contract)
- Balances contract focus with essential implementation details

---

#### Guideline 2: Prioritize Behavioral Workflow Over Superficial Control Interfaces

**Description**

Anything that materially controls system behaviour must be documented, regardless of its file type. Code configuration files (and sometimes externalized scripts) are first-class core components of software [3], and thus deserve the first-class documentation priority. In a multi-turn conversation, one should first let the LLM determine what the core behavioural workflow is, and then extract information and generate documentation for the core files (not limited to source code files). 

**Reasoning**

The ultimate goal for code summarization is for knowledge sharing to avoid silos. However, most documentation techniques and guidelines implicitly treat only source code files as the only target for code documentation generation. However, prior research has shown that configuration files are not just auxiliary, but likely high-dimensional control interfaces that determine system behaviour and performance [1]. This is especially common in ML-based systems, where practitioners rely heavily on externalized scripts and configuration files [2]. As a result, documenting only source code misses the actual locus of ML-based system behaviour. Configuration files are the "first-class component" of software and are "an abstraction level that matches the programming languages used by developers and maintainers" [3], and they deserve the first-class documentation priority. 

**Prompt and Context:**

Prompt 1:
```
You are a documentation assistant for configuration-intensive software systems. Now your task is to identify the systemâ€™s core behavioral workflow before producing any documentation. Read the project under `Week5/artifacts/medical_reasoning_SFT/`, and identify anything that materially controls the system behaviour. 
```

Prompt 2:
```
Read the Python project stored at `./Week5/artifacts/medical_reasoning_SFT`.

This project is configuration-driven: the Python entry point (`main.py`) primarily delegates behavior to Axolotl YAML configuration files located in:

{config_files from the last response}

Your task is to:

1. Identify which artifacts encode the core training behavior.
2. Produce documentation that prioritizes these configuration files as the
   primary behavioral interface.
3. Document `main.py` only insofar as it explains how configurations are selected
   and invoked.

<rule>
Our project's documentation style:
- For code, in the original file:
    @brief [One-line description ending with period]

    [Optional detailed description, 1-2 sentences]

    @param [ name] [Purpose and constraints, not just type]
    @return [What the return value represents]

    @pre [Precondition if any]
    @exceptsafe [No-throw | Strong | Basic]

- For configuration files, in a seperate file:
    - Describe intent, major options, constraints, and interactions.
    - Explain how changes affect model behavior or reproducibility.

</rule>
```

**Task:**

By the first prompt, your task is to communicate with the coding agent and understand the data flow and behaviour workflow of the whole project to determine which files need to be documented. 

By the second prompt, your task is to provide rules and guidelines to the coding agent on how to document code and configuration files by setting separate expectations. 

**Solution or Output generated by GitHub Copilot:**

Answer to prompt 1:

```
1) CLI wrapper script: main.py ðŸ”§

Controls: which config to run, whether to run preprocessing vs training, whether to run with accelerate, and passing a --debug flag.

Key CLI options:
    --config â€” select a config by name (format model_name/config_name, e.g. llama2-chat/lora-openmed_Trinity-Mini)
    --config-path â€” direct YAML path (overrides --config)
    --list â€” list available configs
    --accelerate â€” run via accelerate launch (multi-GPU)
    --preprocess-only â€” run axolotl.cli.preprocess instead of training
    --debug â€” appends --debug to the Axolotl command

Behavioral notes: 
main.py discovers configs under axolotl_configs/*/*.yaml, builds the appropriate axolotl.cli.train or axolotl.cli.preprocess command (or runs via accelerate), and exits with the subprocess return code. It also emits helpful errors for missing config files or missing axolotl installation.

2) Axolotl YAML configs (under axolotl_configs/) â€” primary behavior spec ðŸ“„

Each YAML is the main source of truth for training/preprocessing behavior. Important fields that change system behavior include:

Model & tokenizer:
base_model, model_type, tokenizer_type (choice of base LLM)

Precision / device / memory:
load_in_8bit, load_in_4bit, bf16, tf32, flash_attention, gradient_checkpointing

Dataset settings:
    * datasets: - path: ... (dataset identifier e.g. OpenMed/Medical-Reasoning-SFT-Trinity-Mini)
    * dataset_prepared_path (local prepared dataset location)
    * split
    * type (e.g., chat_template)
    * val_set_size
    * sequence_len
    * sample_packing
    * pad_to_sequence_len

Adapter / LoRA:
    * adapter: lora, lora_r, lora_alpha, lora_dropout, merge_lora, lora_modules_to_save

Training loop / optimizer:
    * micro_batch_size, gradient_accumulation_steps, steps, early_stopping_patience
    * optimizer, learning_rate, lr_scheduler, weight_decay, warmup_steps

Checkpointing / evaluation:
    * eval_steps, save_steps, save_total_limit, load_best_model_at_end, resume_from_checkpoint

Logging / telemetry:
    * wandb_project, wandb_entity, wandb_watch, wandb_name, wandb_log_model

Output and dataset directories:
    * output_dir, dataset_prepared_path

Misc:
    * special_tokens, logging_steps, etc.

These YAML keys directly alter training performance, memory usage, precision, dataset used, checkpointing and experiment tracking.

3) Environment & external dependencies âš™ï¸

Presence or absence of packages changes behavior:
    * axolotl (required; main.py suggests pip install axolotl)
    * accelerate (optional â€” used if --accelerate is passed)
    * Hardware and driver support (CUDA, GPUs) and dtype options (bf16, tf32) affect whether the run succeeds and performance.

4) Dataset availability / layout ðŸ“‚

* datasets entries in YAML (e.g., OpenMed/Medical-Reasoning-SFT-Trinity-Mini) control what data is used.

* dataset_prepared_path points to expected local dataset preparation â€” missing files here will affect preprocess/train steps.

Recommendations for documentation (what to write) ðŸ“
* Explain main.py usage
Example invocations for training, preprocessing, listing configs, and using --accelerate.

* Document config anatomy
A short table of the most important YAML keys and the effect of each (e.g., load_in_8bit â†’ memory tradeoffs; merge_lora â†’ whether LoRA merges into base).

* Document required environment
Required Python packages, GPU requirements, and how bf16/tf32 interact with hardware.

* Dataset notes
Source dataset identifier and expected prepared path; instructions for preparing dataset if absent.

* Troubleshooting
Common errors from missing axolotl, missing YAML path, missing datasets, or unsupported hardware settings.
```

Answer to Prompt 2:

`Week5/artifacts/medical_reasoning_SFT/README.md`:

```
Configuration-first workflow
----------------------------
This project is configuration-driven: the YAML files under `axolotl_configs/` are the primary behavioral interface. See `axolotl_configs/CONFIGS.md` for a detailed reference describing intent, major options, constraints, and how changes affect model behavior and reproducibility.

Common commands
---------------
List available configs:
python main.py --list

Train using a named config (example):
python main.py --config llama2-chat/lora-openmed_Trinity-Mini

Train using a direct YAML path:
python main.py --config-path /path/to/axolotl_configs/llama2-chat/lora-openmed_Trinity-Mini.yaml

Run preprocessing only:
python main.py --config llama2-chat/lora-openmed_Trinity-Mini --preprocess-only

Run with `accelerate` for multi-GPU:
python main.py --config llama2-chat/lora-openmed_Trinity-Mini --accelerate
```

`Week5/artifacts/medical_reasoning_SFT/axolotl_configs/CONFIGS.md`:

```
# Axolotl Config Reference

@brief Reference documentation for Axolotl YAML configs used for Medical-Reasoning-SFT.

This document explains the intent, major options, constraints, and interactions of the YAML config files found in this folder (e.g., `llama2-chat/lora-openmed_Trinity-Mini.yaml` and `qwen2.5-7B/lora-openmed_Trinity-Mini.yaml`). These files are the primary behavioral interface for training and preprocessing in this project.

## Intent
- Provide ready-to-run training/preprocessing configurations for different base models (Llama-2, Qwen).
- Encapsulate dataset selection, model/precision options, LoRA adapter settings, optimizer and scheduler choices, checkpointing/evaluation behavior, and logging/telemetry (e.g., Weights & Biases).

## Major options and their effects
- `base_model`, `model_type`, `tokenizer_type`
  - Intent: Choose the base LLM checkpoint and types used to construct model and tokenizer.
  - Effect: Selecting a different base model changes fundamental model behavior and reproducibility (different weights, tokenization rules).

- `load_in_8bit`, `load_in_4bit`, `bf16`, `tf32`, `flash_attention`, `gradient_checkpointing`
  - Intent: Control memory/precision and performance tradeoffs.
  - Effect: Changing these can enable training on smaller GPUs or change numerical determinism and performance. Use with caution for reproducibility: mixed-precision and quantization can change final model outputs slightly.

- `datasets` / `dataset_prepared_path` / `split` / `type`
  - Intent: Point to the dataset identity (e.g., `OpenMed/Medical-Reasoning-SFT-Trinity-Mini`) and prepared data paths.
  - Effect: These determine training data and validation split, which are the primary factors for model behavior and evaluation. Ensure `dataset_prepared_path` exists or preprocessing is run.

- `sequence_len`, `sample_packing`, `pad_to_sequence_len`
  - Intent: Control tokenization length and batching behavior.
  - Effect: Changes memory use and how examples are truncated/padded; may affect convergence and model behavior on long contexts.

- LoRA adapter settings: `adapter`, `lora_r`, `lora_alpha`, `lora_dropout`, `merge_lora`, `lora_modules_to_save`
  - Intent: Configure LoRA training hyperparameters and checkpointing.
  - Effect: These change the number of trained parameters (and thus expressivity), how adapters are saved, and whether LoRA merges into base weights after training.

- Training loop: `micro_batch_size`, `gradient_accumulation_steps`, `steps`, `early_stopping_patience`
  - Intent: Define effective batch size and training duration.
  - Effect: Changing steps or batch sizes can change model convergence and reproducibility (effective learning rate, total updates).

- Optimization: `optimizer`, `learning_rate`, `lr_scheduler`, `weight_decay`, `warmup_steps`
  - Intent: Controls optimizer algorithm and LR schedule.
  - Effect: Direct impact on training dynamics and final performance; for reproducibility, log exact values and random seeds when relevant.

- Checkpointing/evaluation: `eval_steps`, `save_steps`, `save_total_limit`, `load_best_model_at_end`, `resume_from_checkpoint`
  - Intent: Control how often evaluation and saving occur and which model to pick at the end.
  - Effect: Affects how the "best" model is determined, storage usage, and ability to resume from an intermediate state.

- Logging/telemetry: `wandb_project`, `wandb_entity`, `wandb_watch`, `wandb_name`, `wandb_log_model`
  - Intent: Configure experiment tracking in Weights & Biases.
  - Effect: Affects reproducibility and experiment metadata; sensitive information should not be committed to shared configs.

- `output_dir` and other path fields
  - Intent: Determine where outputs, logs, and adapter checkpoints are saved.
  - Effect: Important for reproducibility and artifact tracking; ensure paths are writable and unique per experiment.

## Constraints and interactions
- `--accelerate` vs local run: Using `accelerate` changes process launching (multi-GPU) and requires `accelerate` to be installed and configured for the target cluster.
- Precision choices (8-bit/4-bit/bf16) interact with `base_model` and hardware capabilities; some models or hardware may not support certain modes.
- LoRA settings and `merge_lora` interact with model saving/serving: merged weights are easier to serve but lose modularity for adapter swapping.
- `dataset_prepared_path` must point to preprocessed data; otherwise run preprocessing (`--preprocess-only`) before training.

## Reproducibility notes
- Always record the config file (exact YAML) used for a run and any `--config-path` overrides.
- Log the `wandb_name` and `output_dir` to link runs to artifacts.
- For exact replication, document random seeds and environment (CUDA/cuDNN versions, PyTorch, transformers, axolotl versions).

## Example: How a change affects behavior
- Increasing `steps` from `1000` to `5000` increases total updates and may improve performance but changes training time and reproducibility unless the config is recorded.
- Switching `load_in_8bit: true` to `load_in_8bit: false` may prevent training on memory-limited GPUs but will change numerical behavior.

## Files covered
- `llama2-chat/lora-openmed_Trinity-Mini.yaml`
- `qwen2.5-7B/lora-openmed_Trinity-Mini.yaml`


> Note: These configs are the authoritative behavioral interface â€” `main.py` is a small CLI wrapper that discovers and invokes these configs with Axolotl. See `../README.md` for quick usage examples.
```

`Week5/artifacts/medical_reasoning_SFT/main.py`:

```
"""
@brief CLI entry point for running Axolotl training and preprocessing via YAML configs.

This module discovers YAML configuration files under `axolotl_configs/*/*.yaml` and
invokes Axolotl's training or preprocessing command with the selected config.

@param args The CLI arguments parsed by argparse. Key flags are:
    --config [name] Select config by key (format: <model>/<config>). Must match a file under `axolotl_configs`.
    --config-path [path] Direct path to a YAML config file (overrides --config). Must exist on disk.
    --list If set, lists available configs and exits.
    --accelerate If set, runs training via `accelerate launch` (for multi-GPU runs).
    --preprocess-only If set, runs `axolotl.cli.preprocess` instead of `axolotl.cli.train`.
    --debug Appends `--debug` to the Axolotl command to enable debug mode in Axolotl.

@return Exits the process with the underlying subprocess return code.

@pre `axolotl` should be installed in the environment (see README). If `--accelerate` is used, `accelerate` should be installed.

@exceptsafe Basic
"""
```

**Why this is better**

1. By this guideline, the developers will be able to get an initial glimpse of the behavioural workflow of the whole project and understand which parts are essential for code comprehension and code summarization.  

2. The point of code summarization is not the action of documenting anything itself, but rather writing a summarization that is comprehensible and concise enough while missing the least amount of essential information for better knowledge sharing. By this guideline, it increases the information density, making the process of code summarization less formalistic. 

---

#### Guideline 3: Focus on Architectural Patterns and Component Interactions, Not Low-Level Implementation (for Problem C)

**Description:**

When documenting code, prioritize high-level abstractions over implementation details by explaining what each component does, why specific design choices were made, how components interact and collaborate, and what data structures and algorithms accomplish. Focus on architectural decisions, API contracts, and the relationships between system components that developers need to understand for maintenance and extension. Avoid line-by-line code explanations, detailed implementation logic (such as exact mathematical formulas), variable-level descriptions, and step-by-step procedural breakdowns, as these obscure the broader system understanding that enables effective development work.

**Reasoning**
Comprehensive API and module documentation for multi-component software architecture presents a unique code summarization challenge because developers must understand not just individual functions, but how entire subsystems interact and compose. Summaries of higher-level code units are highly valuable for quickly gaining a macro-level understanding of various program functionalities [4], which is precisely what multi-component systems demand. In our ride-sharing system, understanding how the RideSharingSystem module orchestrates Driver, Passenger, and Ride components requires summarization at the architectural level, not merely at the method level.

Traditional function-level code summarization fails to capture the essence of multi-component architecture. Summarizing code at class and repository levels is crucial for comprehending complex codebases, as such summarizing encompasses broader context and interactions within the software [5]. A developer working with our system needs to understand that the RideSharingSystem acts as a central orchestrator managing state across multiple components, spatial indexing for location queries, and complex business logic for matching, information that cannot be gleaned from individual method summaries alone.

**Prompt and Context:**

Now, read the following guidelines to solve the task given.
The guideline: Comprehensive API and Module Documentation for Multi-Component Software Architecture.

Your task is to identify:

What each component does (classes, methods, data structures)
â€¢	Why specific design choices were made (e.g., why use a grid-based location system)
â€¢	How different components interact and collaborate (e.g., Driver â†” Ride â†” System)
â€¢	What data structures are used and their purpose (deque for queue, dict for lookups, set for uniqueness)
â€¢	What algorithms accomplish (matching algorithm, surge pricing calculation)
Avoid:
â€¢	Line-by-line code explanations
â€¢	Detailed implementation logic (e.g., exact mathematical formulas in Haversine distance)
â€¢	Variable-level descriptions
â€¢	Step-by-step procedural breakdowns

**Task:** 

**Solution or Output generated by GitHub Copilot:**
**@brief** One-page summary for `ride_sharing_management_system.py`.

Component purposes
- Location: geo container; distances via Haversine (km).
- Driver: operator state (availability, rating, earnings).
- Passenger: rider data (history, payments, preferred drivers).
- Ride: lifecycle and fare/distance record.
- RideSharingSystem: register actors, request/match/start/complete rides.

Design rationale (why)
- Coarse 0.1Â° grid + 5 km radius: cheap locality filter for demo scale.
- Simple scoring (distance, rating, preferred): explainable and testable.
- Surge thresholds are explicit to make demand-supply behavior deterministic.

Interactions (how)
- request_ride -> compute surge -> calculate_fare -> _find_best_driver -> _assign_driver
- start_ride -> sets start_time; complete_ride -> updates earnings, ratings, availability

Data structures (what & why)
- dict: O(1) lookup for drivers/passengers/rides
- deque: queue of pending ride ids (FIFO semantics)
- set: uniqueness for grid cells and preferred drivers

Algorithmic contracts (must be preserved)
- Matching: same VehicleType, distance â‰¤ 5 km, score = 2*(max(0,5-distance)) + rating + (2 if preferred). Highest score wins.
- Surge: 0 drivers â†’ 2.5; demand/available >1â†’1.2; >2â†’1.5; >3â†’2.0.
- Fare: base + distance*per_km + per_minute*(distance/40*60) then * surge; returns rounded fare.

Key invariants & preconditions
- Passenger must be registered before request_ride.
- start_ride must be called before complete_ride (validate start_time).
- Driver availability toggles on assign/complete.

Minimum tests to protect the contract
- Surge thresholds and fare impact tests
- Matching excludes drivers >5 km and honors preferred bonus
- Full lifecycle test: request â†’ match â†’ start â†’ complete (earnings, availability, history)

Short-term fixes (priority)
1. Validate `start_time` in `complete_ride` (raise ValueError).
2. Add driver_id -> cell reverse index to make `_update_driver_grid` O(1).
3. Add deterministic tie-breaker (driver_id) to matching.

...

---

## References

[1] Gong, Jingzhi, and Tao Chen. "Deep configuration performance learning: A systematic survey and taxonomy." ACM Transactions on Software Engineering and Methodology 34.1 (2024): 1-62.

[2] Bhat, Avinash, et al. "Aspirations and practice of ml model documentation: Moving the needle with nudging and traceability." Proceedings of the 2023 CHI conference on human factors in computing systems. 2023.


[3] Ahmmed, Jobayer, Myra B. Cohen, and Paul Gazzillo. "Towards Automated Configuration Documentation." Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 2024.

[5] Sun, Weisong, et al. "Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization." arXiv preprint arXiv:2503.10737 (2025).

[4] Makharev, Vladimir, and Vladimir Ivanov. "Code Summarization Beyond Function Level." 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code). IEEE, 2025.





